"""
kafka/producer.py
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Fetch top-tracks data from Spotify (via ingestion.crawl)
and publish each track record to a Kafka topic.

Requires:
  â€¢ confluent-kafka
  â€¢ ingestion/crawl.py with fetch_artists_and_tracks()
  â€¢ SPOTIFY creds already handled inside crawl.py
"""

import json
import logging
from confluent_kafka import Producer
from ingestion.crawl import fetch_artists_and_tracks  # <-- your crawler wrapper

# â”€â”€â”€ Kafka configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
KAFKA_BOOTSTRAP = "localhost:9092"      # use 'kafka:9092' if running inside Docker
TOPIC_NAME      = "spotify.tracks.raw"

producer_conf = {
    "bootstrap.servers": KAFKA_BOOTSTRAP,
    "client.id": "spotify-producer",
    # Optional: tune batch size / linger for higher throughput
}

# â”€â”€â”€ Delivery-report callback â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def delivery_report(err, msg):
    if err is not None:
        logging.error("âŒ delivery failed: %s", err)
    else:
        logging.info(
            "âœ… delivered key=%s to %s [%d] @ offset %d",
            msg.key(),
            msg.topic(),
            msg.partition(),
            msg.offset()
        )

# â”€â”€â”€ Main workflow â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s | %(levelname)s | %(message)s",
        datefmt="%H:%M:%S",
    )
    logging.info("ðŸš€ starting Spotify â†’ Kafka producer")

    # 1. Fetch fresh data from the crawler
    data = fetch_artists_and_tracks()            # returns {"top_artists": â€¦, "top_tracks": â€¦}
    tracks = data["top_tracks"]
    logging.info("Fetched %d tracks from Spotify API", len(tracks))

    # 2. Produce to Kafka
    producer = Producer(producer_conf)

    for track in tracks:
        # Use track ID as Kafka message key (helps partitioning)
        track_id = track.get("id", "")
        producer.produce(
            TOPIC_NAME,
            key=track_id,
            value=json.dumps(track).encode("utf-8"),
            callback=delivery_report,
        )
        producer.poll(0)        # trigger any queued callbacks immediately

    # 3. Wait for all messages to be delivered
    producer.flush()
    logging.info("ðŸŽ‰ all tracks sent; exiting")

# â”€â”€â”€ Entry-point â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    main()
